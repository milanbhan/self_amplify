{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d6005f-fc08-4abd-8881-66ee0d170497",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "notebook_path = os.getcwd()\n",
    "sys.path.insert(0, notebook_path+'/self_amplify')\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import pandas as pd\n",
    "import self_amplify\n",
    "from self_amplify import CustomWrapper, self_amplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c011e4-1f45-49f5-908c-d0dd3f33f24d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, access_token):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "#     max_memory = \"10000MB\"\n",
    "    max_memory = \"80GB\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "        token = access_token\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name , token = access_token)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749380c6-8f17-4c4f-8e38-096b464bc68f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b18ff4-51ff-465e-a6b9-2a68d9dfae38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e486597cef1c4801ab3330f282ff7bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nOut[5]: GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (1): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (2): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (3): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (4): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (5): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (6): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (7): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (8): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (9): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (10): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (11): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (12): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (13): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (14): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (15): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (16): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n      (17): GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)"
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2b-it'\n",
    "access_token = 'your_HF_access_token",
    "\n",
    "model, tokenizer = load_model(model_name, access_token)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31635c34-6db1-4f17-8435-8dd8d2885e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "arc_df_train = pd.read_csv('/dbfs/mnt/ekixai/main/data/MiB/phd/self_amplify/ARC-Challenge-Train.csv')\n",
    "arc_df_train=arc_df_train[arc_df_train.AnswerKey.isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "arc_df_test = pd.read_csv('/dbfs/mnt/ekixai/main/data/MiB/phd/self_amplify/commonsenseqa_data_train.csv', sep=\";\")\n",
    "arc_df_test=arc_df_test[arc_df_test.AnswerKey.isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "\n",
    "#instantiate the amplifier\n",
    "amplifier = self_amplifier(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984a161a-8081-4a2c-8d67-693bd941c8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\nThis is a big test to simplify the xai function<end_of_turn>\n tensor([[    2,   106,  1645,   108,  1596,   603,   476,  2824,  2121,   577,\n         48180,   573,  1141,  1515,  1411,   107,   108]])\n(4, 14)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This is a big test to simplify the xai function\"\n",
    "template, idx = amplifier.preprocess(text=prompt, with_bracket=False)\n",
    "print(template,idx)\n",
    "print(amplifier.get_index_min_max(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2b0965-bb35-4fc4-991b-6eb84b65c2f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: ['This', 'test', 'the', 'xai']"
     ]
    }
   ],
   "source": [
    "amplifier.generate_with_explanation(model, idx, explainer='LayerDeepLift',\n",
    "                                          topk_words=4, target=None, layer=None, split_dict=None,\n",
    "                                          max_new_tokens=1, temperature=1.0, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6f6239-64e8-4dc8-b2e4-fafa5e372f9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\nB\nA\nA\nOut[9]: [{'role': 'user',\n  'content': \"You are presented with multiple choice question, where choices will look like (A) answer1 (B) answer2 (C) answer3 (D) answer4\\n    generate 4 keywords providing hints and generate the right single answer\\n    Ouput example: The 4 keywords 'word1', 'word2', 'word3', and 'word4' are important to predict that the answer is (A)<eos>\\nBees use pollen from flowers to make honey. Bees help flowers by (A) providing them with fertilizer. (B) helping them grow taller. (C) aiding them in reproduction. (D) increasing photosynthesis.\"},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'honey', 'help', 'flowers', 'reproduction',  suggest that the answer is (C)<eos>\"},\n {'role': 'user',\n  'content': 'The marsh willow herb is a plant native to the northeastern United States. It grows best in damp habitats. Which of the following environmental changes would most likely cause a decrease in the marsh willow herb population in an area? (A) a rainstorm lasting several weeks (B) a drought lasting twelve months (C) unusually low temperatures during the month of July (D) unusually high temperatures during the month of January'},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'changes', 'rainstorm', 'lasting', and 'months' suggest that the answer is (B)<eos>\"},\n {'role': 'user',\n  'content': 'A student is trying to identify a mineral that has a nonmetallic luster and is black. It can also be scratched with a fingernail. According to the mineral reference sheet, the unidentified mineral is most likely (A) mica. (B) magnetite. (C) hornblende. (D) quartz.'},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'fingernail', 'sheet,', 'most', 'hornblende',  suggest that the answer is (A)<eos>\"},\n {'role': 'user',\n  'content': \"Which of these factors affects how fast a sound wave moves? (A) the kind of material it is moving through (B) the amplitude of the sound wave's vibrations (C) the wavelength of the disturbance in the medium (D) the type of motion that caused the sound wave to form\"},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'these', 'affects', 'sound', and 'wavelength' suggest that the answer is (A)<eos>\"}]"
     ]
    }
   ],
   "source": [
    "idx_list_fs = amplifier.generate_context_idx(model, arc_df_train, nb_shot=4, selection_strategy=\"success\")\n",
    "\n",
    "amplifier.generate_in_context_preprompt(model=model, df=arc_df_train, explainer='LayerDeepLift', topk_words=4, \n",
    "                                  idx_list_fs=idx_list_fs, n_steps=3, split_dict=None,\n",
    "                                  target_map_dico=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3eefd9c-beff-4022-bdc6-ac9ea20fc891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\nB\nA\nA\n0\nThe sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? (A) ignore (B) enforce (C) authoritarian (D) yell at (E) avoid\n------------------------------\nexpected anwer: A\nLLM answer LayerDeepLift: \n--------> \"school\", \"efforts\", 'punishing', \"change\", and \"avoid\", suggest  that the correct answer would be B. The word \"enforce\" means to give a law or regulation a legal force.\n-------------------------------\n1\nSammy wanted to go to where the people were.  Where might he go? (A) race track (B) populated areas (C) the desert (D) apartment (E) roadblock\n------------------------------\nexpected anwer: B\nLLM answer LayerDeepLift: \n--------> \"race track\", \"populated areas\", and \"the desert\" suggest  that the correct answer would be ( B) Populated areas.\n-------------------------------\n2\nTo locate a choker not located in a jewelry box or boutique where would you go? (A) jewelry store (B) neck (C) jewlery box (D) jewelry box (E) boutique\n------------------------------\nexpected anwer: A\nLLM answer LayerDeepLift: \n--------> \"neck\", \"jewelry\", 'store', \"box\", and \"boutique\", suggest  that the correct location would be a neck or a place where jewelry is sold.\n-------------------------------\n3\nGoogle Maps and other highway and street GPS services have replaced what? (A) united states (B) mexico (C) countryside (D) atlas (E) oceans\n------------------------------\nexpected anwer: D\nLLM answer LayerDeepLift: \n--------> \"Google\", \"maps\", and \"GPS\" suggest the correct answer to this question.\n-------------------------------\n4\nThe fox walked from the city into the forest, what was it looking for? (A) pretty flowers. (B) hen house (C) natural habitat (D) storybook (E) dense forest\n------------------------------\nexpected anwer: C\nLLM answer LayerDeepLift: \n--------> \"fox\", \"forest\", 'looking', \"natural\", and \"storybook\", suggest  that the correct answer would be option E.\n-------------------------------\n5\nWhat home entertainment equipment requires cable? (A) radio shack (B) substation (C) cabinet (D) television (E) desk\n------------------------------\nexpected anwer: D\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "amplifier.evaluate_fs_with_exp(df_train=arc_df_train, df_test=arc_df_test, model=model, max_new_tokens=50, explainer='LayerDeepLift', idx_list_fs=idx_list_fs,\n",
    "                          topk_words=5, split_dict=None,\n",
    "                          n_steps=3, target_map_dico=None, tokenizer_proxy=None, model_proxy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20431f08-b175-464a-9c80-6b864931e737",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer:  \" food item\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67d8ab0-32ce-4ab3-b0cd-36b5523915ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: [{'role': 'user',\n  'content': \"You are presented with several parts of speech.\\nSummarise what these parts of speech have in common in a very concise way using as few words as possible\\nExample\\n['piano', 'guitar', 'saxophone', 'violin', 'cheyenne', 'drum']\"},\n {'role': 'assistant', 'content': 'Summarization: musical instrument<eos>'},\n {'role': 'user',\n  'content': \"['football', 'basketball', 'baseball', 'tennis', 'badmington', 'soccer']\"},\n {'role': 'assistant', 'content': 'Summarization: sport<eos>'},\n {'role': 'user',\n  'content': \"['lion', 'tiger', 'cat', 'pumas', 'panther', 'leopard']\"},\n {'role': 'assistant', 'content': 'Summarization: feline-type animal<eos>'},\n {'role': 'user',\n  'content': \"['far west', 'cowboy', 'cheyenne', 'gun', 'sherif', 'indians']\"},\n {'role': 'assistant', 'content': 'Summarization: '}]"
     ]
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d787ea82-0673-4a55-8bab-1dcb976f161c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to do : affiner les answer keys pour les enlever des paramètres, en étant dans l'ordre alphabétique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bc61b2-286b-4dd4-9ebc-033df9996edf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fonctions that did not depend on the above class\n",
    "def get_topk_words(attribution, topk, window=(0,0)):\n",
    "        topk_indx = np.sort(np.argpartition(attribution[2], -topk)[-topk:])\n",
    "        top_attribution = [attribution[1][i].replace('\"\\\"',\"\") for i in topk_indx.tolist()]\n",
    "        return(top_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09113619-a92f-4328-ab33-a9f135fd7714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "amplifier = self_amplify(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090cec16-7ba0-40d0-b2b6-6d1e641d378c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 14)\n<bos><start_of_turn>user\nThis is a big test to simplify the xai function<end_of_turn>\n<start_of_turn>model\nThe answer is (\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca741e47-6f88-4c2a-bda4-d822800c00e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<command-2436442466285397>:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  final_attributions = torch.tensor(final_attributions / final_attributions.sum())\nOut[13]: ['This', 'test', 'simplify', 'xai']"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b47a597-1803-4587-8616-4483eecfcf5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arc_df_train = pd.read_csv('/dbfs/mnt/ekixai/main/data/MiB/phd/self_amplify/ARC-Challenge-Train.csv')\n",
    "arc_df_train=arc_df_train[arc_df_train.AnswerKey.isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "arc_df_test = pd.read_csv('/dbfs/mnt/ekixai/main/data/MiB/phd/self_amplify/commonsenseqa_data_train.csv', sep=\";\")\n",
    "arc_df_test=arc_df_test[arc_df_test.AnswerKey.isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "\n",
    "causal_judgment_train = pd.read_csv('/dbfs/mnt/ekixai/main/data/MiB/phd/self_amplify/causal_judgment_train.csv', sep=\";\")\n",
    "causal_judgment_train['len_tokenized'] = causal_judgment_train['question'].apply(lambda t : len(tokenizer.encode(t)))\n",
    "causal_judgment_train = causal_judgment_train[causal_judgment_train['len_tokenized']<512]\n",
    "\n",
    "causal_judgment_test = pd.read_csv('/dbfs/mnt/ekixai/main/data/MiB/phd/self_amplify/causal_judgment_test.csv', sep=\";\")\n",
    "causal_judgment_test['len_tokenized'] = causal_judgment_test['question'].apply(lambda t : len(tokenizer.encode(t)))\n",
    "causal_judgment_test = causal_judgment_test[causal_judgment_test['len_tokenized']<512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d9051f-6b26-4c59-af3d-632a885323be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: [195, 313, 758, 963]"
     ]
    }
   ],
   "source": [
    "amplifier.generate_context_idx(model, arc_df_train, nb_shot=4, selection_strategy=\"success\", answer_keys=['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919bdfe2-4b77-4d24-92d3-383ab1b30e35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n<command-2436442466285397>:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  final_attributions = torch.tensor(final_attributions / final_attributions.sum())\nC\nA\nC\nOut[16]: [{'role': 'user',\n  'content': \"You are presented with multiple choice question, where choices will look like (A) answer1 (B) answer2 (C) answer3 (D) answer4\\n    generate 4 keywords providing hints and generate the right single answer\\n    Ouput example: The 4 keywords 'word1', 'word2', 'word3', and 'word4' are important to predict that the answer is (A)<eos>\\nA student accidentally drops a test tube that breaks when it hits the floor. Which method is the best way to retrieve the broken glass? (A) pick up the pieces with a paper towel (B) use a chemical spill kit (C) use a dustpan and broom (D) pick up the pieces by hand\"},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'glass?', 'paper', 'towel', and 'kit' suggest that the answer is (C)<eos>\"},\n {'role': 'user',\n  'content': 'In whitetail deer, females seldom grow antlers. Which best explains why male whitetail deer grow antlers but females seldom grow antlers? (A) Female deer have no need for antlers. (B) Male deer are older than female deer. (C) Antler growth is controlled by genes. (D) Antler growth depends on behavior.'},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'antlers', 'antlers?', 'controlled', 'genes',  suggest that the answer is (C)<eos>\"},\n {'role': 'user',\n  'content': 'In a healthy forest, dead trees and limbs fall to the ground and decompose. Which of these statements best describes why decomposition is important to a forest ecosystem? (A) Nutrients are released when wood is broken down. (B) Worms produce oxygen used by other organisms. (C) Dead trees provide nest sites for many different species of birds. (D) Water is stored in dead trees and limbs.'},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'why', 'ecosystem?', 'when', 'down',  suggest that the answer is (A)<eos>\"},\n {'role': 'user',\n  'content': 'Which of the following statements best explains why it is warmer at the equator than at the North Pole? (A) The equator has a larger area than the North Pole. (B) The equator is closer to the Sun than the North Pole. (C) The equator receives more direct sunlight than the North Pole. (D) The equator has more hours of daylight per year than the North Pole.'},\n {'role': 'assistant',\n  'content': \"The 4 keywords 'than', 'sunlight', 'North', 'Pole',  suggest that the answer is (C)<eos>\"}]"
     ]
    }
   ],
   "source": [
    "idx_list_fs = amplifier.generate_context_idx(model, arc_df_train, nb_shot=4, selection_strategy=\"success\", answer_keys=['A', 'B', 'C', 'D'])\n",
    "\n",
    "amplifier.generate_in_context_preprompt(model=model, df=arc_df_train, explainer='LayerDeepLift', topk_words=4, \n",
    "                                  idx_list_fs=idx_list_fs, answer_keys=['A', 'B', 'C', 'D'], n_steps=3, split_dict=None,\n",
    "                                  target_map_dico=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8adb2d72-63ef-48f1-81ba-47878c21520b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [{'role': 'user',\n  'content': \"You are presented with multiple choice question, where choices will look like (A) answer1 (B) answer2 (C) answer3 (D) answer4\\n    generate 4 keywords providing hints and generate the right single answer\\n    Ouput example: The 4 keywords 'word1', 'word2', 'word3', and 'word4' are important to predict that the answer is (A)<eos>\\nWhich example describes a behavioral adaptation? (A) A bird builds its nest in the ash near a volcano. (B) A whale has the ability to hold its breath for 20 minutes. (C) A fox's hair is white in the winter and brown in the summer. (D) A monkey has long arms that allow it to swing from one branch to another.\"},\n {'role': 'assistant',\n  'content': '4 important keywords to make the prediction are \"nest\", “ash”, \"volcano”, and “bird”.. Therefore the answer is (A)<eos>'},\n {'role': 'user',\n  'content': 'Water evaporates and falls back to Earth as rain or snow. What is the primary energy source that drives this cycle? (A) The wind (B) The Sun (C) Air pressure (D) Ocean currents'},\n {'role': 'assistant',\n  'content': '4 important keywords to make the prediction are \"Sun\",  \"Water\", “Energy”, and “Cycle”.. Therefore the answer is (B)<eos>'},\n {'role': 'user',\n  'content': 'What precaution must be taken by students when making observations of the Sun? (A) Use only new equipment. (B) Use a very strong telescope. (C) Project image onto a piece of cardboard. (D) Look at the Sun only in the morning.'},\n {'role': 'assistant',\n  'content': '4 important keywords to make the prediction are \"project image on a sheet of white paper\".\\n\\nThe Sun is very bright and can damage the eyes. To protect their eyes, students should use a projector to project an image of a Sun-like object onto paper or a white board. This will make it easier for students to see the image and avoid damaging their eyesight.. Therefore the answer is (C)<eos>'},\n {'role': 'user',\n  'content': 'The gravitational force of the Sun affects the planets in our solar system. Which of these is influenced the most by this force? (A) axial tilt (B) orbital path (C) the masses of the planets (D) the number of moons per planet'},\n {'role': 'assistant',\n  'content': '4 important keywords to make the prediction are \"orbital path\",  \"axial tilt\", “mass of planets”, and “number of moon”.. Therefore the answer is (B)<eos>'}]"
     ]
    }
   ],
   "source": [
    "idx_list_fs = amplifier.generate_context_idx(model, arc_df_train, nb_shot=4, selection_strategy=\"success\", answer_keys=['A', 'B', 'C', 'D'])\n",
    "\n",
    "amplifier.generate_in_context_preprompt(model=model, df=arc_df_train, explainer='self_topk', topk_words=4, \n",
    "                                  idx_list_fs=idx_list_fs, answer_keys=['A', 'B', 'C', 'D'], n_steps=3, split_dict=None,\n",
    "                                  target_map_dico=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397be55c-c2a1-4290-b777-2e71f47c3b47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\nThe sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? (A) ignore (B) enforce (C) authoritarian (D) yell at (E) avoid\n------------------------------\nexpected anwer: A\nLLM answer self_topk: \n--------> \"punishing\", \"efforts\", and \"change\" are key to understanding the context of this question and choosing the correct answer.\n-------------------------------\n1\nSammy wanted to go to where the people were.  Where might he go? (A) race track (B) populated areas (C) the desert (D) apartment (E) roadblock\n------------------------------\nexpected anwer: B\nLLM answer self_topk: \n--------> \"race track\", \"populated areas\", and \"the desert\" are all places where people live. Sammy would likely go somewhere in a populated area, not in the middle of nowhere. Therefore, the correct answer would be ( B) Populated areas.\n-------------------------------\n2\nTo locate a choker not located in a jewelry box or boutique where would you go? (A) jewelry store (B) neck (C) jewlery box (D) jewelry box (E) boutique\n------------------------------\nexpected anwer: A\nLLM answer self_topk: \n--------> \"jewelry store\", \"neck\", 'jewellery box', \"store\", and \"boutique\" are all places where you might find a necklace or other type of jewelry.\n-------------------------------\n3\nGoogle Maps and other highway and street GPS services have replaced what? (A) united states (B) mexico (C) countryside (D) atlas (E) oceans\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"google maps\", \"united states\" , \"mexico\" ,\"countryside\" and \"atlas\" are all important clues that lead to a correct answer.\n\nGoogle maps, along with other GPS (Global Positioning System) services, replaced the need for physical\n-------------------------------\n4\nThe fox walked from the city into the forest, what was it looking for? (A) pretty flowers. (B) hen house (C) natural habitat (D) storybook (E) dense forest\n------------------------------\nexpected anwer: C\nLLM answer self_topk: \n--------> \"forest\", \"natural habitat\", and \"storybook\" provide hints that suggest the fox was looking to find a place to rest or sleep.\n-------------------------------\n5\nWhat home entertainment equipment requires cable? (A) radio shack (B) substation (C) cabinet (D) television (E) desk\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"radio shack\", \"substation\", and \"cable\" are all related to cable TV. Cable TV requires a cable to be connected to a television set in order to receive TV signals.\n-------------------------------\n7\nThe forgotten leftovers had gotten quite old, he found it covered in mold in the back of his what? (A) carpet (B) refrigerator (C) breadbox (D) fridge (E) coach\n------------------------------\nexpected anwer: B\nLLM answer self_topk: \n--------> \"carpet\", \"refrigerator\", and \"coach\" are all related to storing and maintaining food. Therefore, the correct answer would be the letter D, \"fridge\".\n-------------------------------\n9\nWhere is a business restaurant likely to be located? (A) town (B) at hotel (C) mall (D) business sector (E) yellow pages\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"town\", \"mall\", and \"business sector\" are all associated with the location of businesses. A business district is an area of town or city where many businesses are located. Therefore, the correct answer would be  (D).\n-------------------------------\n10\nWhere do you put your grapes just before checking out? (A) mouth (B) grocery cart (C) super market (D) fruit basket (E) fruit market\n------------------------------\nexpected anwer: B\nLLM answer self_topk: \n--------> \"mouth\", \"grocery cart\", and \"super market\" are not relevant to predicting where to put the grapes. Therefore, the correct answer cannot be determined from this context.\n-------------------------------\n11\nBefore getting a divorce, what did the wife feel who was doing all the work? (A) harder (B) anguish (C) bitterness (D) tears (E) sadness\n------------------------------\nexpected anwer: C\nLLM answer self_topk: \n--------> \"harder\", \"anguish\", and \"bitterness\" are all synonyms for a feeling of pain or suffering. Therefore, the correct answer would be  (B).\n-------------------------------\n12\nJohnny sat on a bench and relaxed after doing a lot of work on his hobby.  Where is he? (A) state park (B) bus depot (C) garden (D) gym (E) rest area\n------------------------------\nexpected anwer: C\nLLM answer self_topk: \n--------> \"state park\", \"garden\", and \"rest area\" are all places where people can relax. Johnny sat in a garden, so he must be at a rest  area. Therefore, the correct answer would be  (C).\n-------------------------------\n13\nJames was cooling off two quickly.  He would die if he didn't find some way to stop what? (A) loss of heat (B) revenge (C) expansion (D) relaxation (E) calm down\n------------------------------\nexpected anwer: A\nLLM answer self_topk: \n--------> \"loss of  heat\", \"expansion\", and \"relaxation\" are all related to how heat is transferred. Therefore, the correct answer would be ( C)  expansion.\n-------------------------------\n14\nOf all the rooms in a house it was his favorite, the aromas always drew him to the what? (A) yard (B) basement (C) kitchen (D) living room (E) garden\n------------------------------\nexpected anwer: C\nLLM answer self_topk: \n--------> \"yard\", \"basement\", kitchen\", living  room\", and garden\" are all places where people tend to spend time. His favorite room would be the one that he spent most of his time in. Therefore, answer choice D is correct.\n-------------------------------\n15\nBill is stuck in marsh when a man comes up to him peaking Cajun, where is he? (A) low lands (B) new york (C) forest (D) louisiana (E) everglades\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"low lands\", \"new york\", forest, \"louisiana\", and \"everglades\" are all important clues that point to Bill being in Louisiana.\n-------------------------------\n17\nWhat type of person typically contracts illness? (A) hospital (B) head (C) sick person (D) elderly person (E) doctor's office\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"sick person\", \"disease\", and \"illness\" are most important in generating the correct answer.\n\nAn illness is an abnormal condition that disrupts the normal functioning of an organism. A person who is sick is someone who has an illness. Therefore\n-------------------------------\n18\nWhere would you expect to find a pizzeria while shopping? (A) chicago (B) street (C) little italy (D) food court (E) capital cities\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"Chicago\", \"street\", and \"little Italy\" are all important landmarks in the city of Chicago. Therefore, the correct answer would be A. Chicago\n-------------------------------\n19\nWhen eating everything on the tasting menu, what does one tend to feel? (A) full stomach (B) getting full (C) gaining weight (D) sick (E) satisfaction\n------------------------------\nexpected anwer: B\nLLM answer self_topk: \n--------> \"full stomach\", \"getting full\", and \"gaining weight\" are all related to how the body feels when eating. Therefore, the correct answer would be ( B) Getting full.\n-------------------------------\n21\nWhich entrance would you use if you do not want to use the back entrance? (A) side (B) main (C) anterior (D) current (E) front\n------------------------------\nexpected anwer: B\nLLM answer self_topk: \n--------> \"side\", \"main\", and \"front\" are the most important clues to help you choose the correct answer.\n-------------------------------\n22\nYou can share files with someone if you have a connection to a what? (A) freeway (B) radio (C) wires (D) computer network (E) electrical circuit\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"files\", \"connection\", (radio), (computer network), and (wire) are all related to sharing files.\n-------------------------------\n24\nSean was lying about the body, but he was very scared.  He constantly worried about what? (A) the reward money (B) hurt feelings (C) being found out (D) problems (E) trouble\n------------------------------\nexpected anwer: C\nLLM answer self_topk: \n--------> \"reward money\", \"hurt feelings\", and \"problems\" are relevant to predicting that Sean was worried.\n-------------------------------\n25\nThe drug kingpin told his man to run errands, this was code to go to all the dealers to do what they had? (A) park (B) make time for (C) receive instructions (D) take money (E) leave work\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"park\", \"make time\", and \"receive instructions\" are clues that point to an undercover operation. Drug dealers often use these methods to keep their operations hidden from law enforcement.\n-------------------------------\n26\nThough he could've kept going his body appreciated the rest, it had been constantly what during the day? (A) walk (B) lay down (C) working (D) moving (E) exercise\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"walk\", \"lay down\", and \"working\" are all related to rest.\n\nRest is an activity that involves a decrease in physical activity and a reduction in blood pressure. Walking, lying down and working are examples of rest activities. Therefore, the\n-------------------------------\n27\nToo many people want exotic snakes.  The demand is driving what to carry them? (A) ditch (B) shop (C) north america (D) pet shops (E) outdoors\n------------------------------\nexpected anwer: D\nLLM answer self_topk: \n--------> \"ditch\", \"shop\", and \"pet shops\" are not relevant to predicting that answer. Therefore, the correct answer cannot be determined from this context.\n-------------------------------\n29\nTo prevent any glare during the big football game he made sure to clean the dust of his what? (A) television (B) attic (C) corner (D) they cannot clean corner and library during football match they cannot need that (E) ground\n------------------------------\nexpected anwer: A\nLLM answer self_topk: \n--------> \"television\", \"attic\", and \"corner\" are very important in predicting that answer (a) is correct.\n\nThese words are associated with cleaning dust from surfaces. A television, attic and corner are all places where dust can be found.\n-------------------------------\n30\nI have something in my head I want to share, what ways can I do that? (A) write an essay (B) organize thoughts (C) speak information (D) summarize main points (E) have information\n------------------------------\nexpected anwer: C\nLLM answer self_topk: \n--------> \"write\", \"organize\", and \"speak\" are all related to communication. Therefore, the correct answer would be ( A) Write an Essay.\n-------------------------------\n31\nHe wanted a house that was gated off from other places, where should he start looking? (A) neighborhood (B) subdivision (C) city (D) suburbs (E) street\n------------------------------\nexpected anwer: B\nLLM answer self_topk: \n--------> \"neighborhood\", \"subdivision\", city\", and \"suburbs\" are all important in determining where to look for a gated community. Neighborhood is usually the first place to check, followed by subdivisions and then cities and finally suburbs. Therefore, the correct answer would\n-------------------------------\n32\nWhere in Southern Europe would you find many canals? (A) michigan (B) new york (C) amsterdam (D) venice (E) bridge\n------------------------------\nexpected anwer: D\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_list_fs = amplifier.generate_context_idx(model, arc_df_train, nb_shot=4, selection_strategy=\"success\", answer_keys=['A', 'B', 'C', 'D'])\n",
    "\n",
    "\n",
    "amplifier.evaluate_fs_with_exp(df_train=arc_df_train, df_test=arc_df_test, model=model, max_new_tokens=50, explainer='self_topk', idx_list_fs=idx_list_fs,\n",
    "                          topk_words=5, answer_keys=['A', 'B', 'C', 'D'], split_dict=None,\n",
    "                          n_steps=3, target_map_dico=None, tokenizer_proxy=None, model_proxy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0b38aa-20f9-485c-91b0-fd3b0c798e26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6dff1b6-b440-48f0-8521-d14d627564a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "concepts_list_1 = [\"piano\", \"guitar\", \"saxophone\", \"violin\", \"cheyenne\", \"drum\"]\n",
    "macro_concept_1 = \"musical instrument\"\n",
    "\n",
    "concepts_list_2 = [\"football\", \"basketball\", \"baseball\", \"tennis\", \"badmington\", \"soccer\"]\n",
    "macro_concept_2 = \"sport\"\n",
    "\n",
    "concepts_list_3 = [\"lion\", \"tiger\", \"cat\", \"pumas\", \"panther\", \"leopard\"]\n",
    "macro_concept_3 = \"feline-type animal\"\n",
    "\n",
    "# concepts_list_test = [\"hamburger\", \"hot dog\", \"sandwich\", \"cheeseburger\", \"bigmac\", \"fries\"]\n",
    "\n",
    "#ok\n",
    "concepts_list_test = [\"American Crocodile\", \"Killer Bigfoot\", \"Bigfoot Bonanza\", \"The BLOODY APE\", \"Tiger Shark\", \"Montana legend of mutant/cannibal creatures\", \"Bigfoot\"]\n",
    "\n",
    "#pas ok\n",
    "concepts_list_test = [\"Monster School as a safe haven\", \"The Slender Man Mythos\", \"The Shadow Man\", \"Monster Hunt\", \"Monster\", \"Monster and its intentions\", \"Thed Ude\", \"Living monster made of wood\", \"The Dragon and his activities\", \"The Man\", \"Human monsters and the almighty dollar\", \"Monster Attacks\", \"Monsters\", \"The Science Behind the Monster\", \"Greek mythological monster attack\", \"Frankenstein legend\"]\n",
    " \n",
    "#pas ok\n",
    "concepts_list_test = [\"Deadly witches\", \"Clairvoyance and demonic possession\", \"The Spirits and their Influence\", \"Witchcraft series\", \"Witch Trials\", \"Evil Spirit,Possession\", \"Mediums and spiritual phenomena\", \"Demonic Influence\", \"Voodoo\", \"Witch's Curse\", \"Evil spirit and manifestation\", \"Demonic Possession\", \"The monstrous, vengeful boogie woman\", \"Spirit Possession and Family Conflict\", \"Spirit Possession and Serial Killers\", \"Louis and the witch's spell\", \"Witch Execution\", \"The effects of the curse\", \"Culinary Magic\", \"Murder and Evil Spirits\", \"The curse of Bloody Mary\", \"Magic and power\", \"Magic paw with magical powers\", \"Return of Powerful Witches\", \"Witchcraft and demonic possession\", \"Witchcraft\", \"Coven of witches\", \"Possession and Evil\", \"The ageing Countess\", \"Magical Warlock\", \"Spirit possession\", \"Possessed Inmates\", \"Real-life occult practices\", \"The Magificent Madame Mortem\", \"Magic trick with needle, floss, and pierced ear\", \"EVP and demonic forces\", \"Voodoo and demonic possession\", \"Season of the Witch\", \"Ancient Magic\", \"Witch's potion\", \"Witch hunt\", \"The Usher family curse\", \"Indian sorcerers\", \"Curse of the SUCCUBUS!\", \"Magic\", \"The Curse of Usher\", \"Curse and spirits\", \"Magic and illusion\", \"Witchcraft and necromancy\", \"Witchcraft and Voodoo\", \"Spirit Possession\", \"Personal Demons\", \"Possessed by an evil spirit\", \"Magic and sorcery\", \"Seances\", \"Witch named Leah Smock\", \"Voodoo and the Walking Dead\", \"Witches' Dolls\", \"Deadly turn\", \"Ancient Witch\", \"Adopted daughter becomes possessed by a demonic force\", \"Possession by spirit\", \"Curse\", \"Exorcism gone wrong\", \"Witchcraft accusation\", \"Treatment of Citizens and Black Witch\", \"Demon Possession\", \"Madame Tirelou and her curse\", \"Evil Spirits and Occult History\"]\n",
    "\n",
    "\n",
    "\n",
    "concepts_list_test =  [\"Gold and Treasure\", \"Lost gold mine\", \"The potential exploitation of the valley's silver\", \"The harshness of the frontier\", \"Mine Strike\", \"Banker and secret gold mines\", \"Gold bullion\", \"Gold\", \"California Gold Rush\", \"Wilderness and Greed\", \"Mining\", \"Mining operations\", \"American claims to the Oregon Territory\", \"Discovery of gold\", \"Gold discovery\", \"The decline of the Wild West\", \"A.C. Lyles and his westerns\", \"Silver Mine\", \"Ferguson's abandoned mine\", \"Mine Tunnel\", \"Gold rush\", \"Gold mine\", \"Lost Dutchman mine\", \"Gold-prospecting\", \"Copperminers\", \"Mine value\", \"Gold Creek turmoil\"]\n",
    " \n",
    "concepts_list_test =  [\"Mining operations\", \"Discovery of gold\", \"Gold discovery\", \"The decline of the Wild West\", \"A.C. Lyles and his westerns\",]\n",
    "\n",
    "#ok\n",
    "concepts_list_test = [\"Deadly witches\", \"Clairvoyance and demonic possession\", \"The Spirits and their Influence\", \"Witchcraft series\", \"Witch Trials\", \"Evil Spirit,Possession\", \"Mediums and spiritual phenomena\", \"Demonic Influence\", \"Voodoo\", \"Witch's Curse\", \"Evil spirit and manifestation\", \"Demonic Possession\", \"The monstrous, vengeful boogie woman\", \"Spirit Possession and Family Conflict\", \"Spirit Possession and Serial Killers\", \"Louis and the witch's spell\"]\n",
    "\n",
    "\n",
    "preprompt = '''You are presented with several parts of speech.\n",
    "Summarise what these parts of speech have in common in a very concise way using as few words as possible'''\n",
    "\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": preprompt + \"\\nExample\\n\" + str(concepts_list_1)}]\n",
    "messages.append({\"role\": \"assistant\", \"content\": \"Summarization: \"+macro_concept_1+\"<eos>\"})  \n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\":str(concepts_list_2)})\n",
    "messages.append({\"role\": \"assistant\", \"content\": \"Summarization: \"+macro_concept_2+\"<eos>\"})  \n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\":str(concepts_list_3)})\n",
    "messages.append({\"role\": \"assistant\", \"content\": \"Summarization: \"+macro_concept_3+\"<eos>\"})  \n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\":str(concepts_list_test)})\n",
    "messages.append({\"role\": \"assistant\", \"content\":\"Summarization: \"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7999766f-60c9-46c0-9fdf-224d457e0625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer:  \" supernatural or demonic phenomenon\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.apply_chat_template(messages, return_tensors='pt')\n",
    "\n",
    "encoded_input = torch.reshape(encoded_input[0][:-2], (1, encoded_input[0][:-2].shape[0]))\n",
    "\n",
    "len_input = encoded_input.shape[1]\n",
    "outputs = model.generate(encoded_input.to(device), max_new_tokens = 50, do_sample=True, num_beams=3, no_repeat_ngram_size=2, early_stopping=True, temperature=0.01)\n",
    "answer = '\"' + tokenizer.decode(outputs[0][len_input:], skip_special_tokens=True)\n",
    "print(\"answer: \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0570f73-a578-4dcd-a59e-c31b44e80f39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[94]: 69"
     ]
    }
   ],
   "source": [
    "len([\"Deadly witches\", \"Clairvoyance and demonic possession\", \"The Spirits and their Influence\", \"Witchcraft series\", \"Witch Trials\", \"Evil Spirit Possession\", \"Mediums and spiritual phenomena\", \"Demonic Influence\", \"Voodoo\", \"Witch's Curse\", \"Evil spirit and manifestation\", \"Demonic Possession\", \"The monstrous, vengeful boogie woman\", \"Spirit Possession and Family Conflict\", \"Spirit Possession and Serial Killers\", \"Louis and the witch's spell\", \"Witch Execution\", \"The effects of the curse\", \"Culinary Magic\", \"Murder and Evil Spirits\", \"The curse of Bloody Mary\", \"Magic and power\", \"Magic paw with magical powers\", \"Return of Powerful Witches\", \"Witchcraft and demonic possession\", \"Witchcraft\", \"Coven of witches\", \"Possession and Evil\", \"The ageing Countess\", \"Magical Warlock\", \"Spirit possession\", \"Possessed Inmates\", \"Real-life occult practices\", \"The Magificent Madame Mortem\", \"Magic trick with needle, floss, and pierced ear\", \"EVP and demonic forces\", \"Voodoo and demonic possession\", \"Season of the Witch\", \"Ancient Magic\", \"Witch's potion\", \"Witch hunt\", \"The Usher family curse\", \"Indian sorcerers\", \"Curse of the SUCCUBUS!\", \"Magic\", \"The Curse of Usher\", \"Curse and spirits\", \"Magic and illusion\", \"Witchcraft and necromancy\", \"Witchcraft and Voodoo\", \"Spirit Possession\", \"Personal Demons\", \"Possessed by an evil spirit\", \"Magic and sorcery\", \"Seances\", \"Witch named Leah Smock\", \"Voodoo and the Walking Dead\", \"Witches' Dolls\", \"Deadly turn\", \"Ancient Witch\", \"Adopted daughter becomes possessed by a demonic force\", \"Possession by spirit\", \"Curse\", \"Exorcism gone wrong\", \"Witchcraft accusation\", \"Treatment of Citizens and Black Witch\", \"Demon Possession\", \"Madame Tirelou and her curse\", \"Evil Spirits and Occult History\"]\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1e07d7-df63-4d1f-8543-3858d662554a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[110]: 16"
     ]
    }
   ],
   "source": [
    "len(concepts_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08620b2b-b1ed-4fbf-8370-da1eb25ea026",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo self_amplify_gemma-2B",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
